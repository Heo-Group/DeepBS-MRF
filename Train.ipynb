{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "joohJ90a6vgP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 15:37:05.602328: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import h5py\n",
    "import math\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from scipy.linalg import expm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import inf, nan\n",
    "import time\n",
    "%run my_functions.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro-vLo6U640K",
    "outputId": "0fa0a722-bd80-4640-cdd0-f874600ffa58"
   },
   "outputs": [],
   "source": [
    "## Select GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "tf_device = '/gpu:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t-_U2NKJ68PC"
   },
   "outputs": [],
   "source": [
    "## Data Loading . . . orignally network trained with 40 million dataset\n",
    "\n",
    "# Tissue parameters\n",
    "train_y_file = scipy.io.loadmat ('data/Train_Y.mat')\n",
    "Train_Y = train_y_file['Train_Y']\n",
    "## MTC signal at 1uT\n",
    "mtc_file_1p0_file = scipy.io.loadmat('data/MTC_ref_1p0.mat')\n",
    "MTC_ref_1p0 = mtc_file_1p0_file['MTC_ref_1p0']\n",
    "## MTC signal at 1.5uT\n",
    "mtc_file_1p5_file = scipy.io.loadmat('data/MTC_ref_1p5.mat')\n",
    "MTC_ref_1p5 = mtc_file_1p5_file['MTC_ref_1p5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load trained deep Bloch simulator to generated MTC-MRF signals from tissue parameters\n",
    "model_BS_40DS = tf.keras.models.load_model('saved_models/deep_BS_RNN_40')\n",
    "\n",
    "## Load trained deep Bloch simulator to generated Zref signals at 1uT and 1.5 uT\n",
    "model_BS_2_point = tf.keras.models.load_model('saved_models/deep_BS_2_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler_in = MinMaxScaler()\n",
    "\n",
    "## Scale tissue parameters (0, 1)\n",
    "Train_Y_fit = scaler_in.fit_transform(np.squeeze(Train_Y))\n",
    "Train_X_estimated = model_BS_40DS.predict(np.expand_dims(Train_Y_fit,2))  \n",
    "\n",
    "## Add noise to network simulated model, SNR=46\n",
    "Train_X = mtc_mrf_noisy (np.squeeze(Train_X_estimated), 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A2rVb9LZ8kHh"
   },
   "outputs": [],
   "source": [
    "##   Traning and Testing Split 9:1 ratio\n",
    "split = int(0.9 * len(Train_X))\n",
    "x_train = Train_X[0:split]\n",
    "x_test  = Train_X[split:]\n",
    "GT_train = Train_Y[0:split]          ## kmw, M0m, T2m, T1w\n",
    "GT_test = Train_Y[split:]\n",
    "\n",
    "t1t2w_para_train = GT_train [:,4:5]         ## T1w/T2w\n",
    "t1t2w_para_test = GT_test [:,4:5]    \n",
    "\n",
    "Zref_1p0_train = MTC_ref_1p0[0:split]\n",
    "test_MTC_ref_1p0_test = MTC_ref_1p0[split:]\n",
    "Zref_1p5_train = MTC_ref_1p5[0:split]\n",
    "test_MTC_ref_1p5_test = MTC_ref_1p5[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0h2EBoL5JbLM",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Convert numpy array to tensor\n",
    "x_train = np.expand_dims(x_train,2)\n",
    "GT_train = np.expand_dims(GT_train,2) \n",
    "x_test = np.expand_dims(x_test,2) \n",
    "GT_test = np.expand_dims(GT_test,2) \n",
    "\n",
    "\n",
    "train_input = tf.cast(x_train,  tf.float32)\n",
    "train_target = tf.cast(GT_train,  tf.float32)\n",
    "val_input = tf.cast(x_test,  tf.float32)\n",
    "val_target = tf.cast(GT_test,  tf.float32)\n",
    "\n",
    "train_t2w = tf.cast(t1t2w_para_train,  tf.float32)\n",
    "val_t2w = tf.cast(t1t2w_para_test,  tf.float32)\n",
    "\n",
    "train_zref_1p0 = tf.cast(Zref_1p0_train,  tf.float32)\n",
    "val_zref_1p0 = tf.cast(test_MTC_ref_1p0_test,  tf.float32)\n",
    "\n",
    "train_zref_1p5 = tf.cast(Zref_1p5_train,  tf.float32)\n",
    "val_zref_1p5 = tf.cast(test_MTC_ref_1p5_test,  tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CNN_network(tf.keras.Model):\n",
    "    def __init__(self, n_classes):\n",
    "        super(LSTM_CNN_network, self).__init__()\n",
    "        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, activation='relu', input_shape= (n_classes,1 )))\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')\n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dense_inter = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.dense_out = tf.keras.layers.Dense(4, activation='sigmoid')    \n",
    "        self.pool = tf.keras.layers.MaxPool1D(2)\n",
    "        \n",
    "    def call(self,input, training=False):                     \n",
    "        x = self.bi_lstm(input)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)  ##\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat (x)\n",
    "        x = self.dense_inter(x)\n",
    "        x = self.dense_inter(x)\n",
    "        x = self.dense_inter(x)\n",
    "        x = self.dense_inter(x)\n",
    "        out = self.dense_out(x)\n",
    "        max_4 = tf.constant( [100.0,  0.17,  1e-04,  3.0])  \n",
    "        min_4 = tf.constant( [5.0,    0.02,  1e-06,  0.2])   \n",
    "        out_denorm = tf.math.multiply(out, max_4-min_4) + min_4\n",
    "        \n",
    "        return out_denorm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4ziFRvB7JEh",
    "outputId": "d61235a1-2a1f-4649-a89e-dc0e654d1f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model_recon = LSTM_CNN_network(40)\n",
    "model_recon.build((None,40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x0szP1FSUueP"
   },
   "outputs": [],
   "source": [
    "# Instantiate a loss function.\n",
    "def loss_fn(output, target, signal_out_1, signal_target_1, signal_out_1p5, signal_target_1p5):\n",
    "\n",
    "    max_4 = tf.constant( [100.0,  0.17,  1e-04,  3.0])  \n",
    "    min_4 = tf.constant( [5.0,    0.02,  1e-06,  0.2]) \n",
    "    output_norm = tf.math.divide(output-min_4,max_4-min_4)\n",
    "    target4_norm = tf.math.divide(target-min_4,max_4-min_4)\n",
    "    diff_norm = (output_norm-target4_norm)**2\n",
    "    \n",
    "    diff_signal_1 = (signal_out_1-signal_target_1)**2\n",
    "\n",
    "    diff_signal_1p5 = (signal_out_1p5-signal_target_1p5)**2\n",
    "    \n",
    "    mean_diff_norm = tf.math.reduce_mean(diff_norm)\n",
    "    mean_signal_norm_1 = tf.math.reduce_mean(diff_signal_1)\n",
    "    mean_signal_norm_1p5 = tf.math.reduce_mean(diff_signal_1p5)\n",
    "\n",
    "    error_total = (mean_diff_norm + 30*mean_signal_norm_1 + 15*mean_signal_norm_1p5)\n",
    "    \n",
    "    return error_total, mean_diff_norm, mean_signal_norm_1, mean_signal_norm_1p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XDvtOybaVNd3"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "batch_size_val = 1000\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_target, train_t2w, train_zref_1p0, train_zref_1p5))   \n",
    "train_dataset = train_dataset.shuffle(buffer_size=70000, reshuffle_each_iteration=True).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_input, val_target, val_t2w, val_zref_1p0, val_zref_1p5)) \n",
    "val_dataset = val_dataset.shuffle(buffer_size=70000, reshuffle_each_iteration=False).batch(batch_size_val)\n",
    "\n",
    "learning_rate_fn = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4, decay_steps=10000, decay_rate=0.9)    \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "loss_hist = []\n",
    "loss_hist_para = []\n",
    "loss_hist_zref_1 = []\n",
    "loss_hist_zref_1p5 = []\n",
    "loss_hist_val = []\n",
    "loss_hist_val_para = []\n",
    "loss_hist_val_zref_1 =[]\n",
    "loss_hist_val_zref_1p5 =[]\n",
    "batch_loss = 0 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "    start_time = time.time()\n",
    "    \n",
    "   ################################## training ###########################################\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train, t1w_t2w, MTC_ref_batch_1p0, MTC_ref_batch_1p5) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:        \n",
    "            ## Estimation of four tissue parameters (kmw, M0m, T2m, T1w)\n",
    "            x_pred = model_recon(x_batch_train, training=True)      \n",
    "            ## T2w is known to us\n",
    "            all_tiss = tf.concat((x_pred, t1w_t2w), 1)\n",
    "            ## scaling of all five tissue parameters (0, 1)\n",
    "            all_tiss_scaled = scaler_in.fit_transform(all_tiss)\n",
    "            ## deepBS for Zref(+3.5 ppm) at 1uT and 1.5 uT\n",
    "            MTC_3p5 = model_BS_2_point(tf.expand_dims(all_tiss_scaled,2), training=False) \n",
    "            ## Loss calculation\n",
    "            loss_value, loss_para, loss_z_1, loss_z_1p5 = loss_fn (x_pred, tf.squeeze(y_batch_train[:,0:4,0]), MTC_3p5 [:,0], MTC_ref_batch_1p0, MTC_3p5 [:,1], MTC_ref_batch_1p5)\n",
    "     \n",
    "        grads = tape.gradient(loss_value, model_recon.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model_recon.trainable_weights))\n",
    "        \n",
    " \n",
    "    loss_hist.append(np.mean(loss_value))   \n",
    "    loss_hist_para.append(np.mean(loss_para))   \n",
    "    loss_hist_zref_1.append(np.mean(loss_z_1))   \n",
    "    loss_hist_zref_1p5.append(np.mean(loss_z_1p5))\n",
    "    \n",
    "    print(\"Training loss (for one batch) at step %d: %.4f\"  % (step, float(np.mean(loss_value))) )\n",
    "    print(\"Validation para loss: %.5f\" % (np.mean(loss_para)))\n",
    "    print(\" Zref 1 loss: %.5f\" % (np.mean(loss_z_1)))\n",
    "    print(\" Zref 1.5 loss: %.4f\" % (np.mean(loss_z_1p5)))\n",
    "    \n",
    "   ################################## validation ###########################################\n",
    "   \n",
    "    for x_batch_val, y_batch_val, t1w_t2w_val, MTC_ref_val_1p0, MTC_ref_val_1p5 in val_dataset:\n",
    "        val_data = model_recon(x_batch_val, training=False)\n",
    "        all_tiss_val = tf.concat((val_data, t1w_t2w_val), 1)\n",
    "        all_tiss_val_scaled = scaler_in.fit_transform(all_tiss_val)\n",
    "        MTC_3p5_val = model_BS_2_point(tf.expand_dims(all_tiss_val_scaled,2), training=False) \n",
    "        loss_value_val, loss_para_val, loss_z_1_val, loss_z_1p5_val  = loss_fn (val_data, tf.squeeze(y_batch_val[:,0:4,0]), MTC_3p5_val [:,0], MTC_ref_val_1p0, MTC_3p5_val [:,1], MTC_ref_val_1p5)\n",
    "\n",
    "    loss_hist_val.append(np.mean(loss_value_val))   \n",
    "    loss_hist_val_para.append(np.mean(loss_para_val))   \n",
    "    loss_hist_val_zref_1.append(np.mean(loss_z_1_val))   \n",
    "    loss_hist_val_zref_1p5.append(np.mean(loss_z_1p5_val))\n",
    "    \n",
    "    print(\"Validation loss: %.5f\" % (np.mean(loss_value_val)))\n",
    "    print(\"Validation para loss: %.5f\" % (np.mean(loss_para_val)))\n",
    "    print(\"Validation Zref 1 loss: %.5f\" % (np.mean(loss_z_1_val)))\n",
    "    print(\"Validation Zref 1.5 loss: %.4f\" % (np.mean(loss_z_1p5_val)))\n",
    "    \n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "8m_BiLSTM_CNN two loss_good_30ds.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
